{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c199045",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.document_loaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_experimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticChunker\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.document_loaders'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "–ü—Ä–æ–µ–∫—Ç 7: –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è RAG\n",
    "\n",
    "–ê–≤—Ç–æ—Ä: AI Assistant\n",
    "–û–ø–∏—Å–∞–Ω–∏–µ:\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n",
    "1. –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä\n",
    "2. –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥\n",
    "3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "# ------------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 1: –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö LANGCHAIN\n",
    "# ------------------------------------------------------------------------------\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "print(\"‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 2: –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–ê\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_document(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏–ª–∏ TXT —Ñ–∞–π–ª–∞.\n",
    "\n",
    "    Args:\n",
    "        file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (PDF –∏–ª–∏ TXT)\n",
    "\n",
    "    Returns:\n",
    "        –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "    \"\"\"\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "        text = \"\\n\".join(page.page_content for page in pages)\n",
    "        print(f\"‚úÖ PDF –∑–∞–≥—Ä—É–∂–µ–Ω. –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}\")\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        print(f\"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –†–∞–∑–º–µ—Ä: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "# ------------------------------------------------------------------------------\n",
    "document_text = load_document(\"document.pdf\")\n",
    "\n",
    "print(f\"üìÑ –î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω. –†–∞–∑–º–µ—Ä: {len(document_text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"üìä –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {len(document_text.split())}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 3: –°–¢–†–ê–¢–ï–ì–ò–Ø 1 ‚Äî –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ô –†–ê–ó–ú–ï–†\n",
    "# ------------------------------------------------------------------------------\n",
    "def strategy_1_fixed_size(\n",
    "    text: str,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 0,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Fixed-size chunking.\n",
    "    –ú–æ–∂–µ—Ç —Ä–∞–∑—Ä—ã–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "    \"\"\"\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "chunks_1 = strategy_1_fixed_size(document_text)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 4: –°–¢–†–ê–¢–ï–ì–ò–Ø 2 ‚Äî –†–ï–ö–£–†–°–ò–í–ù–´–ô –ß–ê–ù–ö–ò–ù–ì\n",
    "# ------------------------------------------------------------------------------\n",
    "def strategy_2_recursive(\n",
    "    text: str,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursive chunking ‚Äî –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è RAG.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "chunks_2 = strategy_2_recursive(document_text)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 5: –°–¢–†–ê–¢–ï–ì–ò–Ø 3 ‚Äî –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ß–ê–ù–ö–ò–ù–ì\n",
    "# ------------------------------------------------------------------------------\n",
    "def strategy_3_semantic(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Semantic chunking —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ OpenAI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "        splitter = SemanticChunker(embeddings)\n",
    "        docs = splitter.create_documents([text])\n",
    "        return [doc.page_content for doc in docs]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SemanticChunker error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "chunks_3 = strategy_3_semantic(document_text) if api_key else []\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 6: –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó\n",
    "# ------------------------------------------------------------------------------\n",
    "comparison_data = {\n",
    "    \"–ú–µ—Ç–æ–¥\": [\"–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π\", \"–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π\", \"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π\"],\n",
    "    \"–ö–æ–ª-–≤–æ —á–∞–Ω–∫–æ–≤\": [\n",
    "        len(chunks_1),\n",
    "        len(chunks_2),\n",
    "        len(chunks_3) if chunks_3 else \"N/A\",\n",
    "    ],\n",
    "    \"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä\": [\n",
    "        sum(len(c) for c in chunks_1) / len(chunks_1),\n",
    "        sum(len(c) for c in chunks_2) / len(chunks_2),\n",
    "        sum(len(c) for c in chunks_3) / len(chunks_3) if chunks_3 else \"N/A\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –°–¢–†–ê–¢–ï–ì–ò–ô:\")\n",
    "print(df_comparison)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 7: –ê–ù–ê–õ–ò–ó –°–¢–ê–¢–ò–°–¢–ò–ö–ò\n",
    "# ------------------------------------------------------------------------------\n",
    "def analyze_chunks(chunks: List[str]) -> dict:\n",
    "    if not chunks:\n",
    "        return {}\n",
    "\n",
    "    sizes = [len(c) for c in chunks]\n",
    "    avg = sum(sizes) / len(sizes)\n",
    "\n",
    "    return {\n",
    "        \"total\": len(chunks),\n",
    "        \"avg\": avg,\n",
    "        \"min\": min(sizes),\n",
    "        \"max\": max(sizes),\n",
    "        \"std\": (sum((s - avg) ** 2 for s in sizes) / len(sizes)) ** 0.5,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "print(\"–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π:\", analyze_chunks(chunks_1))\n",
    "print(\"–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π:\", analyze_chunks(chunks_2))\n",
    "print(\"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π:\", analyze_chunks(chunks_3))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# –®–ê–ì 8: –í–´–í–û–î\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\n",
    "    \"\"\"\n",
    "üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:\n",
    "–î–ª—è RAG-—Å–∏—Å—Ç–µ–º –æ–ø—Ç–∏–º–∞–ª–µ–Ω Recursive Chunking:\n",
    "- —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞\n",
    "- –Ω–µ —Ç—Ä–µ–±—É–µ—Ç API\n",
    "- –¥–∞—ë—Ç –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "\n",
    "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "chunk_size = 1000‚Äì1500\n",
    "chunk_overlap = 150‚Äì300\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚ú® –ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù ‚ú®\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
